{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out Hyperspace on MNIST Data - Model Resiliency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperspace import create_hyperspace\n",
    "from ray import tune\n",
    "import tensorflow as tf\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from skopt import Optimizer\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3f68a533b944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _java_worker_options, _code_search_path, _temp_dir, _load_code_from_local, _lru_evict, _metrics_export_port, _object_spilling_config, _system_config)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             raise RuntimeError(\"Maybe you called ray.init twice by accident? \"\n\u001b[0m\u001b[1;32m    648\u001b[0m                                \u001b[0;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                                \u001b[0;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Model Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_tf_objective(config):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(config['dropout']),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    res = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "    res_test = model.evaluate(x_test, y_test)\n",
    "    # res test[0] reports the loss from the evaluation, res_test[1] reports the accuracy\n",
    "    tune.report(test_loss = res_test[0])\n",
    "    return res_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run hypertune for Tensorflow Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-04 23:24:19,077\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5628311634063721 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:24:28,899\tWARNING util.py:136 -- The `process_trial` operation took 8.629640817642212 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:24:29,557\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.652522087097168 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:24:57,448\tWARNING util.py:136 -- The `process_trial` operation took 5.821593284606934 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:25:04,419\tWARNING util.py:136 -- The `process_trial` operation took 6.633246421813965 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:25:10,129\tWARNING util.py:136 -- The `process_trial` operation took 4.040419101715088 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:25:20,410\tWARNING util.py:136 -- The `process_trial` operation took 2.50860333442688 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:25:24,142\tWARNING util.py:136 -- The `process_trial` operation took 3.4611332416534424 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:25:44,321\tWARNING util.py:136 -- The `process_trial` operation took 7.9427831172943115 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:26:09,982\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-04 23:26:09,990\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-04 23:26:11,847\tWARNING util.py:136 -- The `process_trial` operation took 10.058577060699463 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:26:12,503\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.6482048034667969 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:26:26,942\tWARNING util.py:136 -- The `process_trial` operation took 3.510467767715454 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:26:35,929\tWARNING util.py:136 -- The `process_trial` operation took 1.2533040046691895 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:26:36,792\tWARNING util.py:136 -- The `process_trial` operation took 0.7725129127502441 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:29:51,012\tWARNING util.py:136 -- The `process_trial` operation took 8.329748153686523 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:30:07,857\tWARNING util.py:136 -- The `process_trial` operation took 7.577330112457275 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:30:21,662\tWARNING util.py:136 -- The `process_trial` operation took 6.127725839614868 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:30:35,590\tWARNING util.py:136 -- The `process_trial` operation took 3.6501240730285645 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:30:38,729\tWARNING util.py:136 -- The `process_trial` operation took 3.019603967666626 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:30:42,293\tWARNING util.py:136 -- The `process_trial` operation took 3.3400678634643555 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:31:04,841\tWARNING util.py:136 -- The `process_trial` operation took 5.5329930782318115 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:31:11,509\tWARNING util.py:136 -- The `process_trial` operation took 3.874825954437256 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:31:32,779\tWARNING util.py:136 -- The `process_trial` operation took 1.8979671001434326 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:31:36,032\tWARNING util.py:136 -- The `process_trial` operation took 0.5470619201660156 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:31:36,596\tWARNING util.py:136 -- The `process_trial` operation took 0.5156841278076172 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:31,505\tWARNING util.py:136 -- The `process_trial` operation took 5.342796802520752 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:37,630\tWARNING util.py:136 -- The `process_trial` operation took 5.725048065185547 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:38,322\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5990679264068604 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:42,917\tWARNING util.py:136 -- The `process_trial` operation took 4.4166412353515625 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:47,560\tWARNING util.py:136 -- The `process_trial` operation took 3.8961009979248047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:49,838\tWARNING util.py:136 -- The `process_trial` operation took 1.3436610698699951 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:35:51,458\tWARNING util.py:136 -- The `process_trial` operation took 1.5200951099395752 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:36:14,291\tWARNING util.py:136 -- The `process_trial` operation took 5.182107925415039 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:36:51,821\tWARNING util.py:136 -- The `process_trial` operation took 3.7777411937713623 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:36:58,457\tWARNING util.py:136 -- The `process_trial` operation took 1.4396231174468994 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:37:04,929\tWARNING util.py:136 -- The `process_trial` operation took 0.7997970581054688 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:40:25,376\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-04 23:40:27,878\tWARNING util.py:136 -- The `process_trial` operation took 8.112452983856201 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:41:06,448\tWARNING util.py:136 -- The `process_trial` operation took 4.641983985900879 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:41:08,600\tWARNING util.py:136 -- The `process_trial` operation took 1.9515268802642822 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:41:10,678\tWARNING util.py:136 -- The `process_trial` operation took 1.9152212142944336 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:41:13,661\tWARNING util.py:136 -- The `process_trial` operation took 2.888232946395874 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:41:17,510\tWARNING util.py:136 -- The `process_trial` operation took 3.4319841861724854 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:41:50,825\tWARNING util.py:136 -- The `process_trial` operation took 4.503680944442749 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:42:08,089\tWARNING util.py:136 -- The `process_trial` operation took 3.221160888671875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:42:24,488\tWARNING util.py:136 -- The `process_trial` operation took 1.531567096710205 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:42:27,343\tWARNING util.py:136 -- The `process_trial` operation took 0.7308018207550049 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:44:18,242\tWARNING util.py:136 -- The `process_trial` operation took 3.2864041328430176 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:44:21,405\tWARNING util.py:136 -- The `process_trial` operation took 2.9330427646636963 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:44:24,853\tWARNING util.py:136 -- The `process_trial` operation took 3.203758955001831 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:44:33,037\tWARNING util.py:136 -- The `process_trial` operation took 4.606409072875977 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:44:37,472\tWARNING util.py:136 -- The `process_trial` operation took 2.707798957824707 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:44:38,064\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5804319381713867 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:45:03,456\tWARNING util.py:136 -- The `process_trial` operation took 2.849091053009033 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:45:09,000\tWARNING util.py:136 -- The `process_trial` operation took 2.669926881790161 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:45:18,059\tWARNING util.py:136 -- The `process_trial` operation took 1.5072650909423828 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:45:19,507\tWARNING util.py:136 -- The `process_trial` operation took 1.3570046424865723 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:45:23,476\tWARNING util.py:136 -- The `process_trial` operation took 0.5828118324279785 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:46:05,087\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffffe98314f701000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-04 23:46:11,528\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-04 23:46:11,535\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-04 23:46:11,547\tINFO (unknown file):0 -- gc.collect() freed 369 refs in 6.315160202997504 seconds\n",
      "2020-10-04 23:47:24,475\tWARNING util.py:136 -- The `process_trial` operation took 2.73897385597229 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:27,036\tWARNING util.py:136 -- The `process_trial` operation took 2.293851137161255 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:29,791\tWARNING util.py:136 -- The `process_trial` operation took 2.577707052230835 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:33,622\tWARNING util.py:136 -- The `process_trial` operation took 3.7041540145874023 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:39,703\tWARNING util.py:136 -- The `process_trial` operation took 2.7369630336761475 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:42,326\tWARNING util.py:136 -- The `process_trial` operation took 2.459378957748413 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:46,745\tWARNING util.py:136 -- The `process_trial` operation took 1.8878400325775146 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:47:54,219\tWARNING util.py:136 -- The `process_trial` operation took 2.1327710151672363 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:48:00,679\tWARNING util.py:136 -- The `process_trial` operation took 1.7375469207763672 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:48:08,937\tWARNING util.py:136 -- The `process_trial` operation took 0.7812149524688721 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:08,323\tWARNING util.py:136 -- The `process_trial` operation took 4.471805810928345 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:11,376\tWARNING util.py:136 -- The `process_trial` operation took 2.8770480155944824 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:15,519\tWARNING util.py:136 -- The `process_trial` operation took 4.033850908279419 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:16,320\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.6555256843566895 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:20,579\tWARNING util.py:136 -- The `process_trial` operation took 4.15422511100769 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:49,128\tWARNING util.py:136 -- The `process_trial` operation took 1.6754090785980225 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:51,290\tWARNING util.py:136 -- The `process_trial` operation took 2.1403229236602783 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:54,998\tWARNING util.py:136 -- The `process_trial` operation took 3.6250720024108887 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:50:58,997\tWARNING util.py:136 -- The `process_trial` operation took 3.758420944213867 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:51:09,077\tWARNING util.py:136 -- The `process_trial` operation took 1.7060468196868896 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:51:11,850\tWARNING util.py:136 -- The `process_trial` operation took 0.8647387027740479 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:03,099\tWARNING util.py:136 -- The `process_trial` operation took 2.8025519847869873 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:18,212\tWARNING util.py:136 -- The `process_trial` operation took 4.886557102203369 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:28,778\tWARNING util.py:136 -- The `process_trial` operation took 2.757936954498291 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:35,571\tWARNING util.py:136 -- The `process_trial` operation took 4.625687122344971 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:52,700\tWARNING util.py:136 -- The `process_trial` operation took 2.5515568256378174 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:55,268\tWARNING util.py:136 -- The `process_trial` operation took 2.453340768814087 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:57,458\tWARNING util.py:136 -- The `process_trial` operation took 2.129765748977661 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:53:59,931\tWARNING util.py:136 -- The `process_trial` operation took 2.3400139808654785 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:54:00,474\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5000183582305908 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:54:06,285\tWARNING util.py:136 -- The `process_trial` operation took 1.377180814743042 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:54:17,770\tWARNING util.py:136 -- The `process_trial` operation took 0.611238956451416 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:58:43,572\tWARNING util.py:136 -- The `process_trial` operation took 5.549299001693726 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:59:20,419\tWARNING util.py:136 -- The `process_trial` operation took 4.331110000610352 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:59:40,418\tWARNING util.py:136 -- The `process_trial` operation took 2.81968092918396 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-04 23:59:43,142\tWARNING util.py:136 -- The `process_trial` operation took 2.5319459438323975 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:00:52,210\tWARNING util.py:136 -- The `process_trial` operation took 3.4288129806518555 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:00:55,043\tWARNING util.py:136 -- The `process_trial` operation took 2.605924129486084 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:01:09,201\tWARNING util.py:136 -- The `process_trial` operation took 1.5436370372772217 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:01:30,750\tWARNING util.py:136 -- The `process_trial` operation took 1.3031139373779297 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:01:37,366\tWARNING util.py:136 -- The `process_trial` operation took 0.6363708972930908 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:02:54,594\tWARNING util.py:136 -- The `process_trial` operation took 0.5274868011474609 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:03:12,347\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 00:07:36,899\tWARNING util.py:136 -- The `process_trial` operation took 4.1108949184417725 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:07:40,500\tWARNING util.py:136 -- The `process_trial` operation took 2.8245248794555664 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:07:42,344\tWARNING util.py:136 -- The `process_trial` operation took 1.7317709922790527 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:07:44,503\tWARNING util.py:136 -- The `process_trial` operation took 1.8103899955749512 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:09,358\tWARNING util.py:136 -- The `process_trial` operation took 3.657957077026367 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:13,497\tWARNING util.py:136 -- The `process_trial` operation took 3.0330309867858887 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:19,904\tWARNING util.py:136 -- The `process_trial` operation took 3.182403087615967 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:28,587\tWARNING util.py:136 -- The `process_trial` operation took 1.7402701377868652 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:30,671\tWARNING util.py:136 -- The `process_trial` operation took 1.0188632011413574 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:35,087\tWARNING util.py:136 -- The `process_trial` operation took 0.5460069179534912 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:09:57,845\tWARNING util.py:136 -- The `process_trial` operation took 0.9131677150726318 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:15:33,638\tWARNING util.py:136 -- The `process_trial` operation took 4.168337106704712 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:15:51,528\tWARNING util.py:136 -- The `process_trial` operation took 4.1922807693481445 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:15:55,411\tWARNING util.py:136 -- The `process_trial` operation took 3.7440011501312256 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:16:09,798\tWARNING util.py:136 -- The `process_trial` operation took 2.788148880004883 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:17:34,580\tWARNING util.py:136 -- The `process_trial` operation took 3.6564950942993164 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:18:18,318\tWARNING util.py:136 -- The `process_trial` operation took 2.276332139968872 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:18:23,260\tWARNING util.py:136 -- The `process_trial` operation took 2.307931900024414 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:19:02,241\tWARNING util.py:136 -- The `process_trial` operation took 1.3825631141662598 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:19:39,730\tWARNING util.py:136 -- The `process_trial` operation took 0.8208770751953125 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:20:07,592\tWARNING util.py:136 -- The `process_trial` operation took 0.574657678604126 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:23:56,625\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff08cb2f9c01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 00:23:59,351\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 00:23:59,358\tINFO (unknown file):0 -- gc.collect() freed 596 refs in 2.627268908006954 seconds\n",
      "2020-10-05 00:24:14,716\tWARNING util.py:136 -- The `process_trial` operation took 2.6718761920928955 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:25:47,076\tWARNING util.py:136 -- The `process_trial` operation took 4.747941017150879 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:25:50,556\tWARNING util.py:136 -- The `process_trial` operation took 3.34360408782959 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:25:54,958\tWARNING util.py:136 -- The `process_trial` operation took 2.759127140045166 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:26:42,743\tWARNING util.py:136 -- The `process_trial` operation took 3.773155927658081 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:26:48,126\tWARNING util.py:136 -- The `process_trial` operation took 3.0922791957855225 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:27:05,402\tWARNING util.py:136 -- The `process_trial` operation took 1.2873258590698242 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:27:12,543\tWARNING util.py:136 -- The `process_trial` operation took 0.871239185333252 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:28:01,155\tWARNING util.py:136 -- The `process_trial` operation took 0.7351689338684082 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:28:01,897\tWARNING util.py:136 -- The `process_trial` operation took 0.6790030002593994 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:32:01,086\tWARNING util.py:136 -- The `process_trial` operation took 4.066640138626099 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:32:48,993\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 00:32:52,255\tWARNING util.py:136 -- The `process_trial` operation took 8.519766092300415 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:04,207\tWARNING util.py:136 -- The `process_trial` operation took 4.417469024658203 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:10,157\tWARNING util.py:136 -- The `process_trial` operation took 4.439033031463623 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:16,386\tWARNING util.py:136 -- The `process_trial` operation took 3.396587610244751 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:19,362\tWARNING util.py:136 -- The `process_trial` operation took 1.9794921875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:21,048\tWARNING util.py:136 -- The `process_trial` operation took 1.4063639640808105 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:28,121\tWARNING util.py:136 -- The `process_trial` operation took 0.9476659297943115 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:33:57,445\tWARNING util.py:136 -- The `process_trial` operation took 0.9491150379180908 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:36:29,888\tWARNING util.py:136 -- The `process_trial` operation took 4.058076858520508 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:36:41,436\tWARNING util.py:136 -- The `process_trial` operation took 3.1728429794311523 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:36:45,105\tWARNING util.py:136 -- The `process_trial` operation took 3.4140148162841797 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:36:48,990\tWARNING util.py:136 -- The `process_trial` operation took 3.765550136566162 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:37:15,660\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 00:37:15,665\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-05 00:37:17,667\tWARNING util.py:136 -- The `process_trial` operation took 5.572584867477417 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:37:20,844\tWARNING util.py:136 -- The `process_trial` operation took 2.950424909591675 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:37:24,758\tWARNING util.py:136 -- The `process_trial` operation took 1.4687941074371338 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:37:45,814\tWARNING util.py:136 -- The `process_trial` operation took 0.9767308235168457 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:37:55,939\tWARNING util.py:136 -- The `process_trial` operation took 0.6032576560974121 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:39:57,466\tWARNING util.py:136 -- The `process_trial` operation took 0.5343599319458008 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:42:45,422\tWARNING util.py:136 -- The `process_trial` operation took 2.814227819442749 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:43:05,152\tWARNING util.py:136 -- The `process_trial` operation took 4.415621280670166 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:43:12,183\tWARNING util.py:136 -- The `process_trial` operation took 2.9460341930389404 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:43:14,463\tWARNING util.py:136 -- The `process_trial` operation took 1.9256696701049805 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:43:16,786\tWARNING util.py:136 -- The `process_trial` operation took 2.176795721054077 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:43:51,257\tWARNING util.py:136 -- The `process_trial` operation took 3.6593496799468994 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:43:57,212\tWARNING util.py:136 -- The `process_trial` operation took 1.6401469707489014 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:44:04,591\tWARNING util.py:136 -- The `process_trial` operation took 1.4395809173583984 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:44:18,242\tWARNING util.py:136 -- The `process_trial` operation took 0.6624689102172852 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:44:32,034\tWARNING util.py:136 -- The `process_trial` operation took 0.5475199222564697 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:47:03,566\tWARNING util.py:136 -- The `process_trial` operation took 2.778675079345703 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:47:05,571\tWARNING util.py:136 -- The `process_trial` operation took 1.8129830360412598 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:47:07,987\tWARNING util.py:136 -- The `process_trial` operation took 2.182891607284546 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:47:55,919\tWARNING util.py:136 -- The `process_trial` operation took 5.133318901062012 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:48:06,848\tWARNING util.py:136 -- The `process_trial` operation took 3.9884121417999268 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:48:17,262\tWARNING util.py:136 -- The `process_trial` operation took 2.19063401222229 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:48:19,941\tWARNING util.py:136 -- The `process_trial` operation took 2.4988532066345215 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:48:27,305\tWARNING util.py:136 -- The `process_trial` operation took 2.340616226196289 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:48:29,239\tWARNING util.py:136 -- The `process_trial` operation took 0.9260542392730713 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:48:34,916\tWARNING util.py:136 -- The `process_trial` operation took 0.7331521511077881 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 00:49:06,892\tWARNING experiment_analysis.py:533 -- Could not find best trial. Did you pass the correct `metric`parameter?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e78ae5e515de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best config for space \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"avg_test_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "%%capture tf_run_output\n",
    "hyperparameters = [(0.00000001, 0.1),  # learning_rate\n",
    "                   (0.0, 0.9),  # dropout\n",
    "                   (10, 100),  # epochs \n",
    "                   (10, 1000)]  # batch size\n",
    "space = create_hyperspace(hyperparameters)\n",
    "\n",
    "### for each space in hyperspace, we want to search the space using ray tune\n",
    "i = 0\n",
    "results = []\n",
    "for section in tqdm(space):\n",
    "    # create a skopt gp minimize object\n",
    "    optimizer = Optimizer(section)\n",
    "    search_algo = SkOptSearch(optimizer, ['learning_rate', 'dropout', 'epochs', 'batch_size'],\n",
    "                              metric='test_loss', mode='min')\n",
    "    # not using a gpu because running on local\n",
    "    analysis = tune.run(mnist_tf_objective, search_alg=search_algo, num_samples=20)\n",
    "    results.append(analysis)\n",
    "    i += 1\n",
    "\n",
    "# print out the best result\n",
    "i = 0\n",
    "for a in results:\n",
    "    print(\"Best config for space \"+str(i)+\": \"+a.get_best_config(metric=\"avg_test_loss\", mode=\"min\"))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa265ba9dc0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa278fd2490>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa275d32c70>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27db15b80>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27eb6ee80>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27daf78e0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27f01cbb0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27ee3fbe0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27efd1dc0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27f3404f0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa2684b6070>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa279734700>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa26b32ea30>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa26b37d0d0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa26b28d910>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa275e1b580>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Model Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNet(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config['dropout']), \n",
    "            nn.Linear(128, 10), \n",
    "            nn.Softmax())\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.config = config\n",
    "        self.test_loss = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'test_loss': loss}\n",
    "        return {'test_loss': loss, 'logs': logs}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = []\n",
    "        for x in outputs:\n",
    "            loss.append(float(x['test_loss']))\n",
    "        avg_loss = statistics.mean(loss)\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.test_loss = avg_loss\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_pt_objective(config):\n",
    "    model = NumberNet(config)\n",
    "    trainer = pl.Trainer(max_epochs=config['epochs'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    tune.report(test_loss=model.test_loss)\n",
    "    return model.test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-05 01:09:10,616\tWARNING util.py:136 -- The `process_trial` operation took 0.9559600353240967 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:10:40,591\tWARNING util.py:136 -- The `process_trial` operation took 0.6296718120574951 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:13:13,709\tWARNING util.py:136 -- The `process_trial` operation took 0.6996901035308838 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:13:53,407\tWARNING util.py:136 -- The `process_trial` operation took 0.7335710525512695 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:16:11,570\tWARNING util.py:136 -- The `process_trial` operation took 0.6475241184234619 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:19:16,368\tWARNING util.py:136 -- The `process_trial` operation took 0.6921200752258301 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:19:29,851\tWARNING util.py:136 -- The `process_trial` operation took 0.5834648609161377 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:41:23,891\tWARNING util.py:136 -- The `process_trial` operation took 0.6301476955413818 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:43:22,636\tWARNING util.py:136 -- The `process_trial` operation took 0.6802699565887451 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:45:34,432\tWARNING util.py:136 -- The `process_trial` operation took 0.6728758811950684 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:46:14,567\tWARNING util.py:136 -- The `process_trial` operation took 0.7650318145751953 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:46:32,236\tWARNING util.py:136 -- The `process_trial` operation took 0.9175910949707031 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:47:20,990\tWARNING util.py:136 -- The `process_trial` operation took 0.557701826095581 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:50:26,544\tWARNING util.py:136 -- The `process_trial` operation took 0.5620720386505127 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 01:51:25,854\tWARNING util.py:136 -- The `process_trial` operation took 0.6182081699371338 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:10:28,424\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff5fd8ddae01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 02:10:28,782\tINFO (unknown file):0 -- gc.collect() freed 455 refs in 0.2666840870078886 seconds\n",
      "2020-10-05 02:17:00,578\tWARNING util.py:136 -- The `process_trial` operation took 0.7610578536987305 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:19:55,742\tWARNING util.py:136 -- The `process_trial` operation took 0.7539780139923096 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:20:01,279\tWARNING util.py:136 -- The `process_trial` operation took 0.76279616355896 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:21:04,109\tWARNING util.py:136 -- The `process_trial` operation took 0.7623119354248047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:21:18,813\tWARNING util.py:136 -- The `process_trial` operation took 0.7536098957061768 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:51:24,339\tWARNING util.py:136 -- The `process_trial` operation took 0.7189769744873047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:53:03,621\tWARNING util.py:136 -- The `process_trial` operation took 0.6780989170074463 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:55:23,504\tWARNING util.py:136 -- The `process_trial` operation took 0.8643169403076172 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:55:24,656\tWARNING util.py:136 -- The `process_trial` operation took 1.1122448444366455 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:56:02,540\tWARNING util.py:136 -- The `process_trial` operation took 0.8139240741729736 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:56:34,649\tWARNING util.py:136 -- The `process_trial` operation took 0.7158989906311035 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 02:59:25,802\tWARNING util.py:136 -- The `process_trial` operation took 0.6156690120697021 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:14:48,586\tWARNING util.py:136 -- The `process_trial` operation took 0.6149730682373047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:15:24,655\tWARNING util.py:136 -- The `process_trial` operation took 0.7129542827606201 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:17:45,364\tWARNING util.py:136 -- The `process_trial` operation took 0.9590270519256592 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:19:12,140\tWARNING util.py:136 -- The `process_trial` operation took 0.6408951282501221 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:20:31,139\tWARNING util.py:136 -- The `process_trial` operation took 0.5437009334564209 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:21:08,583\tWARNING util.py:136 -- The `process_trial` operation took 0.7138350009918213 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:22:17,168\tWARNING util.py:136 -- The `process_trial` operation took 0.6628918647766113 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:22:37,511\tWARNING util.py:136 -- The `process_trial` operation took 0.532135009765625 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:22:39,212\tWARNING util.py:136 -- The `process_trial` operation took 0.5093388557434082 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:33:11,490\tWARNING util.py:136 -- The `process_trial` operation took 0.8586287498474121 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:33:30,964\tWARNING util.py:136 -- The `process_trial` operation took 0.7897601127624512 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:34:44,704\tWARNING util.py:136 -- The `process_trial` operation took 0.6401550769805908 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:36:07,636\tWARNING util.py:136 -- The `process_trial` operation took 0.7718968391418457 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:36:40,827\tWARNING util.py:136 -- The `process_trial` operation took 0.6318328380584717 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:37:11,285\tWARNING util.py:136 -- The `process_trial` operation took 0.6131870746612549 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:37:36,256\tWARNING util.py:136 -- The `process_trial` operation took 0.5991442203521729 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:48:42,984\tWARNING util.py:136 -- The `process_trial` operation took 0.9803042411804199 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:50:56,518\tWARNING util.py:136 -- The `process_trial` operation took 0.6596119403839111 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:52:43,400\tWARNING util.py:136 -- The `process_trial` operation took 0.7461938858032227 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:52:46,659\tWARNING util.py:136 -- The `process_trial` operation took 0.7239999771118164 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:52:55,213\tWARNING util.py:136 -- The `process_trial` operation took 0.7472779750823975 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:54:03,443\tWARNING util.py:136 -- The `process_trial` operation took 0.6486539840698242 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:54:04,203\tWARNING util.py:136 -- The `process_trial` operation took 0.7012851238250732 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 03:54:39,187\tWARNING util.py:136 -- The `process_trial` operation took 0.535412073135376 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:09:36,566\tWARNING util.py:136 -- The `process_trial` operation took 0.6243619918823242 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:17,919\tWARNING util.py:136 -- The `process_trial` operation took 0.7223358154296875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:30,050\tWARNING util.py:136 -- The `process_trial` operation took 0.8752808570861816 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:40,880\tWARNING util.py:136 -- The `process_trial` operation took 0.8701686859130859 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:12:51,753\tWARNING util.py:136 -- The `process_trial` operation took 0.7167370319366455 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:13:32,144\tWARNING util.py:136 -- The `process_trial` operation took 0.9056692123413086 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:13:40,159\tWARNING util.py:136 -- The `process_trial` operation took 0.6699938774108887 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:13:45,190\tWARNING util.py:136 -- The `process_trial` operation took 0.6128678321838379 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:14:19,644\tWARNING util.py:136 -- The `process_trial` operation took 0.5417137145996094 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:35:01,848\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff0cb5f87f01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 04:35:02,320\tINFO (unknown file):0 -- gc.collect() freed 434 refs in 0.3666324420046294 seconds\n",
      "2020-10-05 04:38:43,132\tINFO (unknown file):0 -- gc.collect() freed 137 refs in 0.3556398209912004 seconds\n",
      "2020-10-05 04:41:14,852\tWARNING util.py:136 -- The `process_trial` operation took 0.7783718109130859 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:46:24,184\tWARNING util.py:136 -- The `process_trial` operation took 0.7167739868164062 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:46:25,022\tINFO (unknown file):0 -- gc.collect() freed 578 refs in 0.28180963201157283 seconds\n",
      "2020-10-05 04:46:32,319\tWARNING util.py:136 -- The `process_trial` operation took 0.699120044708252 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:48:20,170\tWARNING util.py:136 -- The `process_trial` operation took 0.6399269104003906 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:49:52,374\tWARNING util.py:136 -- The `process_trial` operation took 0.8000380992889404 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:49:54,957\tWARNING util.py:136 -- The `process_trial` operation took 0.6914317607879639 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:50:45,114\tWARNING util.py:136 -- The `process_trial` operation took 0.7903120517730713 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:53:49,102\tWARNING util.py:136 -- The `process_trial` operation took 0.6347529888153076 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 04:53:53,995\tWARNING util.py:136 -- The `process_trial` operation took 0.501197099685669 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:19:20,506\tWARNING util.py:136 -- The `process_trial` operation took 0.6543080806732178 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:19:32,056\tWARNING util.py:136 -- The `process_trial` operation took 0.780785083770752 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:22:24,110\tWARNING util.py:136 -- The `process_trial` operation took 0.8712069988250732 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:22:25,211\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff3a208c5701000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 05:22:25,641\tINFO (unknown file):0 -- gc.collect() freed 771 refs in 0.33821831599925645 seconds\n",
      "2020-10-05 05:28:30,069\tWARNING util.py:136 -- The `process_trial` operation took 0.841484785079956 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:29:17,150\tWARNING util.py:136 -- The `process_trial` operation took 0.8014373779296875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:30:30,951\tWARNING util.py:136 -- The `process_trial` operation took 0.5637550354003906 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:31:44,677\tWARNING util.py:136 -- The `process_trial` operation took 0.6907660961151123 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:33:10,746\tWARNING util.py:136 -- The `process_trial` operation took 0.5810654163360596 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 05:33:39,907\tWARNING util.py:136 -- The `process_trial` operation took 0.5638651847839355 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:09:54,358\tWARNING util.py:136 -- The `process_trial` operation took 0.8675358295440674 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:10:28,136\tWARNING util.py:136 -- The `process_trial` operation took 0.7722170352935791 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:11:55,600\tWARNING util.py:136 -- The `process_trial` operation took 0.6198468208312988 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:12:05,434\tWARNING util.py:136 -- The `process_trial` operation took 0.721405029296875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:12:24,239\tWARNING util.py:136 -- The `process_trial` operation took 0.7680249214172363 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:12:41,694\tWARNING util.py:136 -- The `process_trial` operation took 0.6846201419830322 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:13:16,391\tWARNING util.py:136 -- The `process_trial` operation took 0.6668210029602051 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:15:56,662\tWARNING util.py:136 -- The `process_trial` operation took 1.0098509788513184 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:51:46,740\tWARNING util.py:136 -- The `process_trial` operation took 0.7648310661315918 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:52:53,787\tWARNING util.py:136 -- The `process_trial` operation took 0.5007281303405762 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:53:03,351\tWARNING util.py:136 -- The `process_trial` operation took 1.2367639541625977 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:53:13,920\tWARNING util.py:136 -- The `process_trial` operation took 0.7295670509338379 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:53:40,419\tWARNING util.py:136 -- The `process_trial` operation took 0.8262560367584229 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:56:50,346\tWARNING util.py:136 -- The `process_trial` operation took 0.5999209880828857 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:56:55,952\tWARNING util.py:136 -- The `process_trial` operation took 0.5856640338897705 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:56:59,302\tWARNING util.py:136 -- The `process_trial` operation took 0.5626950263977051 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 06:59:38,315\tWARNING util.py:136 -- The `process_trial` operation took 0.5918469429016113 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:18:19,413\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff130030fa01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 07:18:19,857\tINFO (unknown file):0 -- gc.collect() freed 578 refs in 0.3425632010039408 seconds\n",
      "2020-10-05 07:18:33,385\tWARNING util.py:136 -- The `process_trial` operation took 1.0066907405853271 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:18:36,535\tWARNING util.py:136 -- The `process_trial` operation took 1.102480173110962 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:22:06,447\tWARNING util.py:136 -- The `process_trial` operation took 0.6812667846679688 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:22:08,437\tWARNING util.py:136 -- The `process_trial` operation took 1.116215705871582 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:23:20,569\tWARNING util.py:136 -- The `process_trial` operation took 0.7805089950561523 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:23:35,896\tWARNING util.py:136 -- The `process_trial` operation took 0.8241617679595947 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:24:02,374\tWARNING util.py:136 -- The `process_trial` operation took 0.7201240062713623 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:25:55,569\tWARNING util.py:136 -- The `process_trial` operation took 0.6736791133880615 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:28:38,430\tWARNING util.py:136 -- The `process_trial` operation took 0.619905948638916 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:34:03,293\tWARNING util.py:136 -- The `process_trial` operation took 0.5171501636505127 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:47:28,331\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff937342cd01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 07:47:28,802\tINFO (unknown file):0 -- gc.collect() freed 401 refs in 0.3670527409994975 seconds\n",
      "2020-10-05 07:49:11,719\tWARNING util.py:136 -- The `process_trial` operation took 0.8393349647521973 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:49:32,100\tWARNING util.py:136 -- The `process_trial` operation took 0.7972688674926758 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:52:23,378\tWARNING util.py:136 -- The `process_trial` operation took 0.9997148513793945 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:53:16,625\tWARNING util.py:136 -- The `process_trial` operation took 0.8392829895019531 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:53:59,406\tWARNING util.py:136 -- The `process_trial` operation took 0.7867043018341064 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:54:06,122\tWARNING util.py:136 -- The `process_trial` operation took 0.8255581855773926 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:55:03,341\tWARNING util.py:136 -- The `process_trial` operation took 0.7078230381011963 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:55:57,459\tWARNING util.py:136 -- The `process_trial` operation took 0.6324379444122314 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 07:56:36,342\tWARNING util.py:136 -- The `process_trial` operation took 0.5371367931365967 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:15:57,227\tWARNING util.py:136 -- The `process_trial` operation took 0.7875690460205078 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:02,146\tWARNING util.py:136 -- The `process_trial` operation took 0.8702259063720703 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:36,165\tWARNING util.py:136 -- The `process_trial` operation took 0.7490520477294922 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:37,015\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff102d304001000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-05 08:16:37,472\tINFO (unknown file):0 -- gc.collect() freed 771 refs in 0.36160219201701693 seconds\n",
      "2020-10-05 08:16:41,351\tWARNING util.py:136 -- The `process_trial` operation took 0.7966089248657227 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:16:50,153\tWARNING util.py:136 -- The `process_trial` operation took 0.8274497985839844 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:19:05,432\tWARNING util.py:136 -- The `process_trial` operation took 0.8022730350494385 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:19:35,379\tWARNING util.py:136 -- The `process_trial` operation took 0.7854599952697754 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:20:51,941\tWARNING util.py:136 -- The `process_trial` operation took 0.5797548294067383 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:21:03,578\tWARNING util.py:136 -- The `process_trial` operation took 0.5127899646759033 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:21:34,671\tWARNING util.py:136 -- The `process_trial` operation took 0.5252151489257812 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:35:56,032\tWARNING util.py:136 -- The `process_trial` operation took 0.6913552284240723 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:38:28,825\tWARNING util.py:136 -- The `process_trial` operation took 0.6818523406982422 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:38:45,495\tWARNING util.py:136 -- The `process_trial` operation took 0.7610533237457275 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:39:52,357\tWARNING util.py:136 -- The `process_trial` operation took 0.5770130157470703 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:40:33,309\tWARNING util.py:136 -- The `process_trial` operation took 0.5916500091552734 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:42:55,115\tWARNING util.py:136 -- The `process_trial` operation took 0.5290818214416504 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:43:05,435\tWARNING util.py:136 -- The `process_trial` operation took 0.5578219890594482 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:43:28,530\tWARNING util.py:136 -- The `process_trial` operation took 0.6276211738586426 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-05 08:46:13,880\tWARNING experiment_analysis.py:533 -- Could not find best trial. Did you pass the correct `metric`parameter?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-75ac5b7c8f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best config for space \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"avg_test_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "%%capture tf_run_output\n",
    "hyperparameters = [(0.00000001, 0.1),  # learning_rate\n",
    "                   (0.0, 0.9),  # dropout\n",
    "                   (10, 100),  # epochs \n",
    "                   (10, 1000)]  # batch size\n",
    "space = create_hyperspace(hyperparameters)\n",
    "\n",
    "### for each space in hyperspace, we want to search the space using ray tune\n",
    "i = 0\n",
    "results = []\n",
    "for section in tqdm(space):\n",
    "    # create a skopt gp minimize object\n",
    "    optimizer = Optimizer(section)\n",
    "    search_algo = SkOptSearch(optimizer, ['learning_rate', 'dropout', 'epochs', 'batch_size'],\n",
    "                              metric='test_loss', mode='min')\n",
    "    # not using a gpu because running on local\n",
    "    analysis = tune.run(mnist_pt_objective, search_alg=search_algo, num_samples=20)\n",
    "    results.append(analysis)\n",
    "    i += 1\n",
    "\n",
    "# print out the best result\n",
    "i = 0\n",
    "for a in results:\n",
    "    print(\"Best config for space \"+str(i)+\": \"+a.get_best_config(metric=\"avg_test_loss\", mode=\"min\"))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27719a370>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27e7fa640>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa21657c550>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa275e1b700>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa2177df040>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27ed31b80>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa217654c70>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa26b24e1f0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa2177ac5e0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27ed38f40>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27ed38160>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa26b366070>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa225b6edf0>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27e8c3f40>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa224350a60>,\n",
       " <ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fa27f00d640>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-39c852965aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_pt_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "all_pt_results = pt_results[0].results_df\n",
    "for i in range(1, len(pt_results)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
