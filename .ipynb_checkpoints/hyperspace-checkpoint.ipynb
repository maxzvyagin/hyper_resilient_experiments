{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out Hyperspace on MNIST Data - Model Resiliency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from hyperspace import create_hyperspace\n",
    "from ray import tune\n",
    "import tensorflow as tf\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from skopt import Optimizer\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 15:12:29,685\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.240',\n",
       " 'raylet_ip_address': '192.168.1.240',\n",
       " 'redis_address': '192.168.1.240:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-10-01_15-12-29_198752_57535/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-10-01_15-12-29_198752_57535/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-10-01_15-12-29_198752_57535',\n",
       " 'metrics_export_port': 60139}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Model Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_tf_objective(config):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(config['dropout']),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    res = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "    res_test = model.evaluate(x_test, y_test)\n",
    "    # res test[0] reports the loss from the evaluation, res_test[1] reports the accuracy\n",
    "    tune.report(test_loss = res_test[0])\n",
    "    return res_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run hypertune for Tensorflow Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 15:12:36,727\tWARNING function_runner.py:485 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2020-10-01 15:14:40,408\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffffbd5c534001000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 2.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 2 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-01 15:14:43,896\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-01 15:14:43,897\tINFO (unknown file):0 -- gc.collect() freed 103 refs in 3.3938123600000267 seconds\n",
      "2020-10-01 15:14:43,901\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-01 15:16:40,850\tWARNING util.py:136 -- The `process_trial` operation took 6.2466700077056885 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:16:53,386\tWARNING util.py:136 -- The `process_trial` operation took 6.158079147338867 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:16:59,902\tWARNING util.py:136 -- The `process_trial` operation took 6.1351940631866455 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:17:03,813\tWARNING util.py:136 -- The `process_trial` operation took 3.369593858718872 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:17:06,532\tWARNING util.py:136 -- The `process_trial` operation took 2.608814001083374 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:17:24,246\tWARNING util.py:136 -- The `process_trial` operation took 3.921454668045044 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:17:38,901\tWARNING util.py:136 -- The `process_trial` operation took 4.973574876785278 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:17:51,340\tWARNING util.py:136 -- The `process_trial` operation took 3.5776760578155518 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:18:19,198\tWARNING util.py:136 -- The `process_trial` operation took 2.0662970542907715 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:18:27,788\tWARNING util.py:136 -- The `process_trial` operation took 1.1311638355255127 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:18:30,391\tWARNING util.py:136 -- The `process_trial` operation took 0.588778018951416 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:21:54,465\tWARNING util.py:136 -- The `process_trial` operation took 5.646331787109375 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:21:58,747\tWARNING util.py:136 -- The `process_trial` operation took 3.657550096511841 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:22:03,753\tWARNING util.py:136 -- The `process_trial` operation took 4.849946975708008 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:22:08,972\tWARNING util.py:136 -- The `process_trial` operation took 5.05898118019104 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:22:32,254\tWARNING util.py:136 -- The `process_trial` operation took 4.764168977737427 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:22:52,469\tWARNING util.py:136 -- The `process_trial` operation took 7.510253190994263 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:22:56,712\tWARNING util.py:136 -- The `process_trial` operation took 3.991777181625366 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:22:57,229\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5166890621185303 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:23:30,291\tWARNING util.py:136 -- The `process_trial` operation took 3.61183500289917 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:23:34,098\tWARNING util.py:136 -- The `process_trial` operation took 2.003156900405884 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:23:43,780\tWARNING util.py:136 -- The `process_trial` operation took 1.659311056137085 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:23:44,905\tWARNING util.py:136 -- The `process_trial` operation took 0.6134929656982422 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:36,994\tWARNING util.py:136 -- The `process_trial` operation took 4.439032793045044 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:39,155\tWARNING util.py:136 -- The `process_trial` operation took 1.6819992065429688 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:40,393\tWARNING util.py:136 -- The `process_trial` operation took 1.101149082183838 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:41,141\tWARNING util.py:136 -- The `process_trial` operation took 0.7007710933685303 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:42,545\tWARNING util.py:136 -- The `process_trial` operation took 1.2750489711761475 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:44,142\tWARNING util.py:136 -- The `process_trial` operation took 1.2295219898223877 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:27:57,932\tWARNING util.py:136 -- The `process_trial` operation took 3.6333460807800293 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:28:42,123\tWARNING util.py:136 -- The `process_trial` operation took 4.8674798011779785 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:28:51,344\tWARNING util.py:136 -- The `process_trial` operation took 1.3733651638031006 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:28:55,158\tWARNING util.py:136 -- The `process_trial` operation took 0.7435810565948486 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:28:55,896\tWARNING util.py:136 -- The `process_trial` operation took 0.6782779693603516 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:32:57,848\tWARNING util.py:136 -- The `process_trial` operation took 5.5585901737213135 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:32:58,562\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.7070987224578857 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:02,495\tWARNING util.py:136 -- The `process_trial` operation took 3.6314852237701416 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:07,705\tWARNING util.py:136 -- The `process_trial` operation took 3.5224173069000244 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:10,317\tWARNING util.py:136 -- The `process_trial` operation took 2.4117820262908936 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:10,821\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5004568099975586 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:13,313\tWARNING util.py:136 -- The `process_trial` operation took 2.098661184310913 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:14,745\tWARNING util.py:136 -- The `process_trial` operation took 1.3445839881896973 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:33:48,978\tWARNING util.py:136 -- The `process_trial` operation took 4.353826999664307 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:34:42,586\tWARNING util.py:136 -- The `process_trial` operation took 2.3772389888763428 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:34:44,036\tWARNING util.py:136 -- The `process_trial` operation took 1.2738890647888184 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:34:45,143\tWARNING util.py:136 -- The `process_trial` operation took 0.8031060695648193 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:34:48,581\tWARNING util.py:136 -- The `process_trial` operation took 0.71573805809021 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:20,899\tWARNING util.py:136 -- The `process_trial` operation took 7.302254915237427 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:21,601\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.6771190166473389 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:30,066\tWARNING util.py:136 -- The `process_trial` operation took 4.830971002578735 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:34,813\tWARNING util.py:136 -- The `process_trial` operation took 3.921628952026367 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:39,390\tWARNING util.py:136 -- The `process_trial` operation took 4.020241975784302 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:42,184\tWARNING util.py:136 -- The `process_trial` operation took 2.720634698867798 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:37:57,692\tWARNING util.py:136 -- The `process_trial` operation took 2.7761919498443604 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:38:05,075\tWARNING util.py:136 -- The `process_trial` operation took 2.963136911392212 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:38:09,831\tWARNING util.py:136 -- The `process_trial` operation took 2.355221748352051 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:38:14,956\tWARNING util.py:136 -- The `process_trial` operation took 1.4046878814697266 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:38:18,185\tWARNING util.py:136 -- The `process_trial` operation took 0.7518401145935059 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:39:07,337\tWARNING util.py:136 -- The `process_trial` operation took 0.8848099708557129 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:00,981\tWARNING util.py:136 -- The `process_trial` operation took 5.320898771286011 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:18,995\tWARNING util.py:136 -- The `process_trial` operation took 5.820367097854614 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:24,177\tWARNING util.py:136 -- The `process_trial` operation took 5.038228988647461 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:29,082\tWARNING util.py:136 -- The `process_trial` operation took 4.168370962142944 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:37,836\tWARNING util.py:136 -- The `process_trial` operation took 3.6765899658203125 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:41,823\tWARNING util.py:136 -- The `process_trial` operation took 3.4943161010742188 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:40:48,346\tWARNING util.py:136 -- The `process_trial` operation took 3.014504909515381 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:41:16,018\tWARNING util.py:136 -- The `process_trial` operation took 2.577584743499756 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:41:17,432\tWARNING util.py:136 -- The `process_trial` operation took 1.308488130569458 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:41:18,049\tWARNING util.py:136 -- The `process_trial` operation took 0.5737180709838867 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:42:06,073\tWARNING util.py:136 -- The `process_trial` operation took 1.5185248851776123 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:43:18,917\tWARNING util.py:136 -- The `process_trial` operation took 2.1688778400421143 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:43:48,221\tWARNING util.py:136 -- The `process_trial` operation took 3.7207419872283936 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:43:52,358\tWARNING util.py:136 -- The `process_trial` operation took 3.223655939102173 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:14,679\tWARNING util.py:136 -- The `process_trial` operation took 5.388660430908203 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:30,639\tWARNING util.py:136 -- The `process_trial` operation took 4.716590881347656 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:36,567\tWARNING util.py:136 -- The `process_trial` operation took 3.738982677459717 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:37,130\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.5572628974914551 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:42,031\tWARNING util.py:136 -- The `process_trial` operation took 4.865730047225952 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:45,631\tWARNING util.py:136 -- The `process_trial` operation took 3.4793238639831543 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:54,926\tWARNING util.py:136 -- The `process_trial` operation took 1.5466678142547607 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:44:56,765\tWARNING util.py:136 -- The `process_trial` operation took 0.9922828674316406 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:45:05,231\tWARNING util.py:136 -- The `process_trial` operation took 0.5459880828857422 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:47:38,612\tWARNING util.py:136 -- The `process_trial` operation took 6.94560980796814 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:47:39,289\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.671807050704956 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:47:48,781\tWARNING util.py:136 -- The `process_trial` operation took 3.698843002319336 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:02,651\tWARNING util.py:136 -- The `process_trial` operation took 4.420324802398682 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:03,621\tWARNING util.py:136 -- The `experiment_checkpoint` operation took 0.9656157493591309 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:18,662\tWARNING util.py:136 -- The `process_trial` operation took 2.61483097076416 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:22,341\tWARNING util.py:136 -- The `process_trial` operation took 3.491440773010254 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:25,604\tWARNING util.py:136 -- The `process_trial` operation took 3.218744993209839 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:31,407\tWARNING util.py:136 -- The `process_trial` operation took 4.372181177139282 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:48,112\tWARNING util.py:136 -- The `process_trial` operation took 4.009593963623047 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:52,917\tWARNING util.py:136 -- The `process_trial` operation took 2.5224759578704834 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:48:56,198\tWARNING util.py:136 -- The `process_trial` operation took 1.4387059211730957 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:49:01,872\tWARNING util.py:136 -- The `process_trial` operation took 0.6378631591796875 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:53:36,216\tWARNING worker.py:945 -- The driver may not be able to keep up with the stdout/stderr of the workers. To avoid forwarding logs to the driver, use 'ray.init(log_to_driver=False)'.\n",
      "2020-10-01 15:53:38,782\tWARNING util.py:136 -- The `process_trial` operation took 7.037084341049194 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:54:08,941\tWARNING util.py:136 -- The `process_trial` operation took 3.8558053970336914 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:54:14,533\tWARNING util.py:136 -- The `process_trial` operation took 2.4309909343719482 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:54:24,459\tWARNING util.py:136 -- The `process_trial` operation took 2.816106081008911 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:54:51,771\tWARNING util.py:136 -- The `process_trial` operation took 4.098958253860474 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:55:40,552\tWARNING util.py:136 -- The `process_trial` operation took 2.7637948989868164 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:55:43,807\tWARNING util.py:136 -- The `process_trial` operation took 1.6997549533843994 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:55:45,368\tWARNING util.py:136 -- The `process_trial` operation took 1.4480576515197754 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:56:09,429\tWARNING util.py:136 -- The `process_trial` operation took 1.012294054031372 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 15:56:55,261\tWARNING util.py:136 -- The `process_trial` operation took 0.8200788497924805 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:02:34,748\tWARNING worker.py:1072 -- The actor or task with ID ffffffffffffffff6f8d59fd01000000 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {CPU: 1.000000}, {object_store_memory: 1.708984 GiB}, {memory: 5.029297 GiB}, {node:192.168.1.240: 1.000000}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "2020-10-01 16:02:35,740\tINFO (unknown file):0 -- gc.collect() freed 297 refs in 0.8725385370003096 seconds\n",
      "2020-10-01 16:03:22,161\tWARNING util.py:136 -- The `process_trial` operation took 4.5800158977508545 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:03:48,681\tWARNING util.py:136 -- The `process_trial` operation took 4.717700719833374 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:03:53,100\tWARNING util.py:136 -- The `process_trial` operation took 4.286961078643799 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:04:57,593\tWARNING util.py:136 -- The `process_trial` operation took 4.189468145370483 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:05:02,344\tWARNING util.py:136 -- The `process_trial` operation took 4.48520302772522 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:05:06,581\tWARNING util.py:136 -- The `process_trial` operation took 3.7797300815582275 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:05:24,115\tWARNING util.py:136 -- The `process_trial` operation took 2.4025051593780518 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:05:26,280\tWARNING util.py:136 -- The `process_trial` operation took 2.073399782180786 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:06:23,540\tWARNING util.py:136 -- The `process_trial` operation took 0.9913091659545898 seconds to complete, which may be a performance bottleneck.\n",
      "2020-10-01 16:06:24,943\tWARNING util.py:136 -- The `process_trial` operation took 0.8166608810424805 seconds to complete, which may be a performance bottleneck.\n"
     ]
    }
   ],
   "source": [
    "%%capture tf_run_output\n",
    "hyperparameters = [(0.00000001, 0.1),  # learning_rate\n",
    "                   (0.0, 0.9),  # dropout\n",
    "                   (10, 100),  # epochs \n",
    "                   (10, 1000)]  # batch size\n",
    "space = create_hyperspace(hyperparameters)\n",
    "\n",
    "### for each space in hyperspace, we want to search the space using ray tune\n",
    "i = 0\n",
    "results = []\n",
    "for section in tqdm(space):\n",
    "    # create a skopt gp minimize object\n",
    "    optimizer = Optimizer(section)\n",
    "    search_algo = SkOptSearch(optimizer, ['learning_rate', 'dropout', 'epochs', 'batch_size'],\n",
    "                              metric='test_loss', mode='min')\n",
    "    # not using a gpu because running on local\n",
    "    analysis = tune.run(mnist_tf_objective, search_alg=search_algo, num_samples=20)\n",
    "    results.append(analysis)\n",
    "    i += 1\n",
    "\n",
    "# print out the best result\n",
    "i = 0\n",
    "for a in results:\n",
    "    print(\"Best config for space \"+str(i)+\": \"+a.get_best_config(metric=\"avg_test_loss\", mode=\"min\"))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Model Objective Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNet(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config['dropout']), \n",
    "            nn.Linear(128, 10), \n",
    "            nn.Softmax())\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.config = config\n",
    "        self.test_loss = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'test_loss': loss}\n",
    "        return {'test_loss': loss, 'logs': logs}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = []\n",
    "        for x in outputs:\n",
    "            loss.append(float(x['test_loss']))\n",
    "        avg_loss = statistics.mean(loss)\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.test_loss = avg_loss\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_pt_objective(config):\n",
    "    model = NumberNet(config)\n",
    "    trainer = pl.Trainer(max_epochs=config['epochs'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    tune.report(test_loss=model.test_loss)\n",
    "    return model.test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 14:48:43,414\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86ed9d7e: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56945, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56945, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:49:05,204\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86effd6c: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56950, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56950, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:50:55,373\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86eeb7f4: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56946, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56946, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:53:35,851\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86f0e542: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56953, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56953, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:54:07,634\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86efa768: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56949, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56949, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:54:08,429\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86ee36b2: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56947, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56947, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:54:56,186\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86ef3a08: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56948, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56948, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:56:27,922\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86f05910: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56951, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56951, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 14:57:54,560\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86f153ce: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56983, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56983, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n",
      "2020-10-01 15:00:28,325\tERROR trial_runner.py:567 -- Trial mnist_pt_objective_86f1a54a: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 515, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 488, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56981, ip=192.168.1.240)\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trainable.py\", line 336, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 340, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 458, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=56981, ip=192.168.1.240)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 227, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 289, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 497, in _trainable_func\n",
      "    output = train_func(config)\n",
      "  File \"<ipython-input-33-ea5ba4ab88d6>\", line 5, in mnist_pt_objective\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1355, in test\n",
      "    results = self.__test_given_model(model, test_dataloaders)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1418, in __test_given_model\n",
      "    results = self.fit(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n",
      "    result = fn(self, *args, **kwargs)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1084, in fit\n",
      "    results = self.accelerator_backend.train(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_backend.py\", line 39, in train\n",
      "    results = self.trainer.run_pretrain_routine(model)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1209, in run_pretrain_routine\n",
      "    eval_loop_results, _ = self.run_evaluation(test_mode=True)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 582, in run_evaluation\n",
      "    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 396, in _evaluate\n",
      "    eval_results = self.__run_eval_epoch_end(test_mode, outputs, dataloaders, using_eval_result)\n",
      "  File \"/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 469, in __run_eval_epoch_end\n",
      "    eval_results = model.test_epoch_end(eval_results)\n",
      "  File \"<ipython-input-32-3aed9b4866f9>\", line 50, in test_epoch_end\n",
      "NameError: name 'statistics' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-75ac5b7c8f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                               metric='test_loss', mode='min')\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# not using a gpu because running on local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_pt_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_algo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73:  20%|██        | 22/109 [00:03<00:11,  7.32it/s, loss=1.907, v_num=0]\n",
      "Epoch 59:  54%|█████▎    | 36/67 [00:08<00:07,  4.38it/s, loss=1.881, v_num=0]\n",
      "Epoch 55:  62%|██████▏   | 57/92 [00:09<00:05,  5.96it/s, loss=1.611, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "%%capture tf_run_output\n",
    "hyperparameters = [(0.00000001, 0.1),  # learning_rate\n",
    "                   (0.0, 0.9),  # dropout\n",
    "                   (10, 100),  # epochs \n",
    "                   (10, 1000)]  # batch size\n",
    "space = create_hyperspace(hyperparameters)\n",
    "\n",
    "### for each space in hyperspace, we want to search the space using ray tune\n",
    "i = 0\n",
    "results = []\n",
    "for section in tqdm(space):\n",
    "    # create a skopt gp minimize object\n",
    "    optimizer = Optimizer(section)\n",
    "    search_algo = SkOptSearch(optimizer, ['learning_rate', 'dropout', 'epochs', 'batch_size'],\n",
    "                              metric='test_loss', mode='min')\n",
    "    # not using a gpu because running on local\n",
    "    analysis = tune.run(mnist_pt_objective, search_alg=search_algo, num_samples=20)\n",
    "    results.append(analysis)\n",
    "    i += 1\n",
    "\n",
    "# print out the best result\n",
    "i = 0\n",
    "for a in results:\n",
    "    print(\"Best config for space \"+str(i)+\": \"+a.get_best_config(metric=\"avg_test_loss\", mode=\"min\"))\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
