{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Distribution Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'learning_rate': .001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_weights(config):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(config['dropout']),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    res = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "    res_test = model.evaluate(x_test, y_test)\n",
    "    just_tf_weights = list()\n",
    "    # get weights\n",
    "    for w in model.weights:\n",
    "        just_tf_weights.extend(w.numpy().flatten())\n",
    "    # scale the weights\n",
    "    scaled_weights = MinMaxScaler().fit_transform(np.array(just_tf_weights).reshape(-1, 1))+1\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNet(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config['dropout']), \n",
    "            nn.Linear(128, 10)) ### no softmax because it's included in cross entropy loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.config = config\n",
    "        self.test_loss = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'test_loss': loss}\n",
    "        return {'test_loss': loss, 'logs': logs}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = []\n",
    "        for x in outputs:\n",
    "            loss.append(float(x['test_loss']))\n",
    "        avg_loss = statistics.mean(loss)\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.test_loss = avg_loss\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pt_weights(config):\n",
    "    model = NumberNet(config)\n",
    "    trainer = pl.Trainer(max_epochs=config['epochs'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    pt_model_weights = list(model.parameters())\n",
    "    just_pt_weights = list()\n",
    "    for w in pt_model_weights:\n",
    "        just_pt_weights.extend(w.detach().numpy().flatten())\n",
    "    pt_weights_scaled = MinMaxScaler().fit_transform(np.array(just_pt_weights).reshape(-1, 1))+1\n",
    "    return pt_weights_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3350 - accuracy: 0.9039\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1609 - accuracy: 0.9527\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1224 - accuracy: 0.9630\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1002 - accuracy: 0.9701\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0856 - accuracy: 0.9733\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0725 - accuracy: 0.9778\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0647 - accuracy: 0.9788\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0572 - accuracy: 0.9815\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0528 - accuracy: 0.9826\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0489 - accuracy: 0.9842\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0445 - accuracy: 0.9857\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0402 - accuracy: 0.9865\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0382 - accuracy: 0.9876\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0360 - accuracy: 0.9877\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9882\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0310 - accuracy: 0.9897\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9901\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0288 - accuracy: 0.9900\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0272 - accuracy: 0.9909\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9911\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0258 - accuracy: 0.9907\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9912\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0217 - accuracy: 0.9927\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0237 - accuracy: 0.9920\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9917\n",
      "313/313 [==============================] - 0s 586us/step - loss: 0.0769 - accuracy: 0.9807\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3445 - accuracy: 0.9003\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1665 - accuracy: 0.9515\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.9630\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1012 - accuracy: 0.9692\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0856 - accuracy: 0.9733\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0740 - accuracy: 0.9775\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0646 - accuracy: 0.9795\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0594 - accuracy: 0.9817\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0550 - accuracy: 0.9827: 0s - loss: 0.0554 - accuracy: \n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0488 - accuracy: 0.9843\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0449 - accuracy: 0.9852\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0442 - accuracy: 0.9857\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0387 - accuracy: 0.9873: 0s - loss: 0.0391 - accura\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0369 - accuracy: 0.9880\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0337 - accuracy: 0.9891\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0335 - accuracy: 0.9888\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0312 - accuracy: 0.9895\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0292 - accuracy: 0.9903\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0275 - accuracy: 0.9910\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0273 - accuracy: 0.9906\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9916\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9915\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0234 - accuracy: 0.9919\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9925\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0226 - accuracy: 0.9923: 0s - loss: 0.0214 - accu\n",
      "313/313 [==============================] - 0s 632us/step - loss: 0.0815 - accuracy: 0.9784\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3374 - accuracy: 0.9035\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1636 - accuracy: 0.9517\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1231 - accuracy: 0.9636\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0989 - accuracy: 0.9705\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0838 - accuracy: 0.9746\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0735 - accuracy: 0.9769: 0s - loss: 0.0721 - accuracy\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0643 - accuracy: 0.9797\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.9817\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0519 - accuracy: 0.9829\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0473 - accuracy: 0.9854\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0436 - accuracy: 0.9859\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0424 - accuracy: 0.9858\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0376 - accuracy: 0.9875\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0354 - accuracy: 0.9878\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0323 - accuracy: 0.9893\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0309 - accuracy: 0.9896\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0297 - accuracy: 0.9899\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0289 - accuracy: 0.9905\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0272 - accuracy: 0.9910\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0251 - accuracy: 0.9914\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0251 - accuracy: 0.9918\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0234 - accuracy: 0.9920\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9919\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9927\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0218 - accuracy: 0.9930\n",
      "313/313 [==============================] - 0s 599us/step - loss: 0.0791 - accuracy: 0.9787\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3456 - accuracy: 0.9004\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1641 - accuracy: 0.9520\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.9633\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0990 - accuracy: 0.9698\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0818 - accuracy: 0.9752\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0726 - accuracy: 0.9769\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0612 - accuracy: 0.9809\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0576 - accuracy: 0.9817: 0s - loss: 0.054\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0514 - accuracy: 0.9835: 0s - loss: 0.0514 - accuracy: 0.98\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0473 - accuracy: 0.9844\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0431 - accuracy: 0.9858\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0379 - accuracy: 0.9881\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0373 - accuracy: 0.9877\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0349 - accuracy: 0.9884\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0305 - accuracy: 0.9899\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0318 - accuracy: 0.9890\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0291 - accuracy: 0.9902\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0278 - accuracy: 0.9908\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0276 - accuracy: 0.9908\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9918\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0257 - accuracy: 0.9915\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9923\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9925\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9925\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9927\n",
      "313/313 [==============================] - 0s 795us/step - loss: 0.0718 - accuracy: 0.9809\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3388 - accuracy: 0.9028\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1650 - accuracy: 0.9513\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1254 - accuracy: 0.9629\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1026 - accuracy: 0.9691\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0861 - accuracy: 0.9737\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0757 - accuracy: 0.9767\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0661 - accuracy: 0.9793\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0598 - accuracy: 0.9812\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0542 - accuracy: 0.9827\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0498 - accuracy: 0.9836\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0461 - accuracy: 0.9851\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0415 - accuracy: 0.9870\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0385 - accuracy: 0.9872\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0367 - accuracy: 0.9878\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0353 - accuracy: 0.9883\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0326 - accuracy: 0.9890\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0298 - accuracy: 0.9898\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0289 - accuracy: 0.9902\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0276 - accuracy: 0.9909\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9904\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0254 - accuracy: 0.9914\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9918\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0244 - accuracy: 0.9917\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0232 - accuracy: 0.9915\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0224 - accuracy: 0.9926\n",
      "313/313 [==============================] - 0s 719us/step - loss: 0.0762 - accuracy: 0.9805\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3418 - accuracy: 0.9010\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1604 - accuracy: 0.9533\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.9648\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0975 - accuracy: 0.9704\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0811 - accuracy: 0.9754\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0726 - accuracy: 0.9775\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0649 - accuracy: 0.9797\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0570 - accuracy: 0.9818\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0511 - accuracy: 0.9836\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0479 - accuracy: 0.9849\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0439 - accuracy: 0.9856\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0406 - accuracy: 0.9867\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0392 - accuracy: 0.9870\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0352 - accuracy: 0.9884\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0335 - accuracy: 0.9890\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0314 - accuracy: 0.9898\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0291 - accuracy: 0.9901\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0281 - accuracy: 0.9905\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0279 - accuracy: 0.9906\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0248 - accuracy: 0.9916\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0257 - accuracy: 0.9912\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9916\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9924\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9920\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9926\n",
      "313/313 [==============================] - 0s 695us/step - loss: 0.0745 - accuracy: 0.9821\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3444 - accuracy: 0.9001\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1648 - accuracy: 0.9507\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1239 - accuracy: 0.9629\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1012 - accuracy: 0.9698: 0s - loss: 0.1004 - accu\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0862 - accuracy: 0.9730\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0756 - accuracy: 0.9766\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0673 - accuracy: 0.9796\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.9819\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0535 - accuracy: 0.9831\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0486 - accuracy: 0.9837\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0452 - accuracy: 0.9849\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0410 - accuracy: 0.9865\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0390 - accuracy: 0.9872\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0377 - accuracy: 0.9870\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0337 - accuracy: 0.9884\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0298 - accuracy: 0.9899\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0312 - accuracy: 0.9894\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0284 - accuracy: 0.9902\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0280 - accuracy: 0.9901\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0271 - accuracy: 0.9909\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9909\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0241 - accuracy: 0.9920\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0242 - accuracy: 0.9912\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0225 - accuracy: 0.9923\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9926\n",
      "313/313 [==============================] - 0s 649us/step - loss: 0.0777 - accuracy: 0.9810\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3363 - accuracy: 0.9026\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1626 - accuracy: 0.9520\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9651\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1000 - accuracy: 0.9700\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0840 - accuracy: 0.9751\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0719 - accuracy: 0.9782\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0641 - accuracy: 0.9798: 0s - loss: 0.0634 - ac\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0562 - accuracy: 0.9820\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0501 - accuracy: 0.9839\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0477 - accuracy: 0.9847\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0412 - accuracy: 0.9862: \n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0404 - accuracy: 0.9870\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0375 - accuracy: 0.9877\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0342 - accuracy: 0.9888\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0321 - accuracy: 0.9892\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0302 - accuracy: 0.9900\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0298 - accuracy: 0.9900\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0272 - accuracy: 0.9909\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0249 - accuracy: 0.9919\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9912\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0245 - accuracy: 0.9916\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0243 - accuracy: 0.9919\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9926\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0206 - accuracy: 0.9933\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0204 - accuracy: 0.9933\n",
      "313/313 [==============================] - 0s 613us/step - loss: 0.0780 - accuracy: 0.9809\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.9047\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1595 - accuracy: 0.9537: 0s - loss: 0.1609 - accuracy: \n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.9646\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0981 - accuracy: 0.9707\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0811 - accuracy: 0.9752: 0s -\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0715 - accuracy: 0.9775\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0642 - accuracy: 0.9794\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0566 - accuracy: 0.9824\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0515 - accuracy: 0.9838\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0464 - accuracy: 0.9847\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0431 - accuracy: 0.9855\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0395 - accuracy: 0.9874\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0368 - accuracy: 0.9880\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0352 - accuracy: 0.9887\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0315 - accuracy: 0.9891\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0295 - accuracy: 0.9900\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9908\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0283 - accuracy: 0.9906\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0275 - accuracy: 0.9909\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0261 - accuracy: 0.9912\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0246 - accuracy: 0.9918\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9925\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0228 - accuracy: 0.9923\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0212 - accuracy: 0.9925\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9926\n",
      "313/313 [==============================] - 0s 580us/step - loss: 0.0775 - accuracy: 0.9812\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3424 - accuracy: 0.9018\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1657 - accuracy: 0.9510\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.9632\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0992 - accuracy: 0.9699\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0849 - accuracy: 0.9734\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0741 - accuracy: 0.9768\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0658 - accuracy: 0.9794\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0584 - accuracy: 0.9811\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0520 - accuracy: 0.9834\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0507 - accuracy: 0.9832\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0452 - accuracy: 0.9854: 0s - loss: 0.0\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0413 - accuracy: 0.9867\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0383 - accuracy: 0.9872\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0356 - accuracy: 0.9880\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0343 - accuracy: 0.9888\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0327 - accuracy: 0.9889\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0300 - accuracy: 0.9898\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0305 - accuracy: 0.9897\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0257 - accuracy: 0.9916\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0247 - accuracy: 0.9917\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9912\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0242 - accuracy: 0.9916\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0227 - accuracy: 0.9925\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9926\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9926\n",
      "313/313 [==============================] - 0s 766us/step - loss: 0.0831 - accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "all_tf_weights = list()\n",
    "for i in range(10):\n",
    "    all_tf_weights.append(get_tf_weights(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tf_weights = np.mean(all_tf_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 4/938 [00:00<00:31, 29.43it/s, loss=2.227, v_num=23]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:09<00:00, 93.92it/s, loss=0.023, v_num=23] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:09<00:00, 93.89it/s, loss=0.023, v_num=23]\n",
      "Testing:   2%|▏         | 17/938 [00:00<00:05, 164.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 933/938 [00:05<00:00, 170.08it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.006322422512482839, 'test_loss': 0.006322422512482839}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 173.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   1%|▏         | 14/938 [00:00<00:10, 88.58it/s, loss=2.056, v_num=24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:13<00:00, 67.67it/s, loss=0.024, v_num=24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:13<00:00, 67.61it/s, loss=0.024, v_num=24]\n",
      "Testing:   1%|▏         | 12/938 [00:00<00:08, 111.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 929/938 [00:08<00:00, 110.99it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.007391922231718301, 'test_loss': 0.007391922231718301}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:08<00:00, 113.19it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 9/938 [00:00<00:13, 68.29it/s, loss=2.149, v_num=26]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 92.55it/s, loss=0.020, v_num=26] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 92.45it/s, loss=0.020, v_num=26]\n",
      "Testing:   2%|▏         | 15/938 [00:00<00:06, 145.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  98%|█████████▊| 923/938 [00:06<00:00, 169.16it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.007391686294188387, 'test_loss': 0.007391686294188387}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:06<00:00, 153.89it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 16/938 [00:00<00:09, 101.38it/s, loss=1.998, v_num=27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 92.85it/s, loss=0.015, v_num=27] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 92.75it/s, loss=0.015, v_num=27]\n",
      "Testing:   2%|▏         | 19/938 [00:00<00:05, 181.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 933/938 [00:05<00:00, 139.06it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.0068636377817224295, 'test_loss': 0.0068636377817224295}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:06<00:00, 155.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   2%|▏         | 16/938 [00:00<00:08, 104.07it/s, loss=2.035, v_num=28]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:08<00:00, 108.98it/s, loss=0.022, v_num=28]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:08<00:00, 108.93it/s, loss=0.022, v_num=28]\n",
      "Testing:   2%|▏         | 20/938 [00:00<00:04, 189.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████▉| 936/938 [00:05<00:00, 165.44it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.008644895186758976, 'test_loss': 0.008644895186758976}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 171.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   2%|▏         | 16/938 [00:00<00:10, 90.73it/s, loss=2.017, v_num=32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 86.74it/s, loss=0.016, v_num=32] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 86.72it/s, loss=0.016, v_num=32]\n",
      "Testing:   2%|▏         | 18/938 [00:00<00:05, 172.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 929/938 [00:06<00:00, 186.94it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.005707650057138574, 'test_loss': 0.005707650057138574}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:06<00:00, 153.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   2%|▏         | 17/938 [00:00<00:08, 106.91it/s, loss=1.938, v_num=35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:11<00:00, 81.50it/s, loss=0.020, v_num=35] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:11<00:00, 81.48it/s, loss=0.020, v_num=35]\n",
      "Testing:   2%|▏         | 16/938 [00:00<00:06, 153.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 930/938 [00:05<00:00, 174.09it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.00758886054557349, 'test_loss': 0.00758886054557349}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 171.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   2%|▏         | 17/938 [00:00<00:08, 106.86it/s, loss=1.980, v_num=37]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:09<00:00, 97.40it/s, loss=0.013, v_num=37]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:09<00:00, 97.37it/s, loss=0.013, v_num=37]\n",
      "Testing:   2%|▏         | 19/938 [00:00<00:05, 183.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 928/938 [00:05<00:00, 187.28it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.006858440873819841, 'test_loss': 0.006858440873819841}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 184.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   2%|▏         | 16/938 [00:00<00:09, 100.63it/s, loss=1.986, v_num=40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  51%|█████▏    | 481/938 [00:05<00:05, 86.81it/s, loss=0.035, v_num=40] "
     ]
    }
   ],
   "source": [
    "all_pt_weights = list()\n",
    "for i in range(10):\n",
    "    all_pt_weights.append(get_pt_weights(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pt_weights = np.mean(all_pt_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence(mean_tf_weights, mean_pt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
