{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Distribution Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config = {'learning_rate': .001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_weights(config):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(config['dropout']),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    res = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=config['batch_size'])\n",
    "    res_test = model.evaluate(x_test, y_test)\n",
    "    just_tf_weights = list()\n",
    "    # get weights\n",
    "    for w in model.weights:\n",
    "        just_tf_weights.extend(w.numpy().flatten())\n",
    "    # scale the weights\n",
    "    scaled_weights = MinMaxScaler().fit_transform(np.array(just_tf_weights).reshape(-1, 1))+1\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNet(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(784, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(config['dropout']), \n",
    "            nn.Linear(128, 10)) ### no softmax because it's included in cross entropy loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.config = config\n",
    "        self.test_loss = None\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(torchvision.datasets.MNIST(\"~/resiliency/\", train=True, \n",
    "                                                                      transform=torchvision.transforms.ToTensor(), target_transform=None, download=True), \n",
    "                                           batch_size=int(self.config['batch_size']))\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        logs = {'test_loss': loss}\n",
    "        return {'test_loss': loss, 'logs': logs}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = []\n",
    "        for x in outputs:\n",
    "            loss.append(float(x['test_loss']))\n",
    "        avg_loss = statistics.mean(loss)\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.test_loss = avg_loss\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pt_weights(config):\n",
    "    model = NumberNet(config)\n",
    "    trainer = pl.Trainer(max_epochs=config['epochs'])\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    pt_model_weights = list(model.parameters())\n",
    "    just_pt_weights = list()\n",
    "    for w in pt_model_weights:\n",
    "        just_pt_weights.extend(w.detach().numpy().flatten())\n",
    "    pt_weights_scaled = MinMaxScaler().fit_transform(np.array(just_pt_weights).reshape(-1, 1))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.9054\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1613 - accuracy: 0.9535\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1202 - accuracy: 0.9651\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.9711\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0821 - accuracy: 0.9745\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0715 - accuracy: 0.9777\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0646 - accuracy: 0.9795\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0564 - accuracy: 0.9821\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.9840\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0489 - accuracy: 0.9839\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.9860\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0397 - accuracy: 0.9868\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9874: 0s - loss:\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0344 - accuracy: 0.9893\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9885\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9901\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0278 - accuracy: 0.9908\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0290 - accuracy: 0.9896\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0255 - accuracy: 0.9912\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9909\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0242 - accuracy: 0.9918\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0242 - accuracy: 0.9916\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0217 - accuracy: 0.9922\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0214 - accuracy: 0.9929\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9931\n",
      "313/313 [==============================] - 0s 719us/step - loss: 0.0753 - accuracy: 0.9821\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3364 - accuracy: 0.9032\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1621 - accuracy: 0.9524\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9638\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1002 - accuracy: 0.9705\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9743: 0s - loss: 0.0839 - accuracy: 0.97\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.9780\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0652 - accuracy: 0.9795\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0576 - accuracy: 0.9817\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0527 - accuracy: 0.9834\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0473 - accuracy: 0.9845\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9855\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.9857\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0381 - accuracy: 0.9875\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9883\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0336 - accuracy: 0.9890\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0318 - accuracy: 0.9895\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9905\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9911\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9907\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0260 - accuracy: 0.9912\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9920\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0237 - accuracy: 0.9922\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0230 - accuracy: 0.9923\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0218 - accuracy: 0.9928\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9923\n",
      "313/313 [==============================] - 0s 798us/step - loss: 0.0798 - accuracy: 0.9802\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3398 - accuracy: 0.9026\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1624 - accuracy: 0.9520\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9638\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0979 - accuracy: 0.9709\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9736\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0736 - accuracy: 0.9773\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0658 - accuracy: 0.9789\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0582 - accuracy: 0.9817\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9838\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0491 - accuracy: 0.9840\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0452 - accuracy: 0.9854\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9861\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9879\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9874\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0345 - accuracy: 0.9883\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9892\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9896\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0283 - accuracy: 0.9903\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0279 - accuracy: 0.9905: 0s - loss: 0.0280 - accura\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9910\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9919\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0247 - accuracy: 0.9920\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0222 - accuracy: 0.9922\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9924\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0227 - accuracy: 0.9920: 0s - loss: 0.0228 - accuracy: 0.\n",
      "313/313 [==============================] - 0s 665us/step - loss: 0.0818 - accuracy: 0.9817\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3397 - accuracy: 0.9028\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1653 - accuracy: 0.9523\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9642\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0998 - accuracy: 0.9696\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0841 - accuracy: 0.9741\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.9775\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0642 - accuracy: 0.9802\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0574 - accuracy: 0.9824\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0527 - accuracy: 0.9830\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0480 - accuracy: 0.9844\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0445 - accuracy: 0.9859\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9867\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9882\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9878\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9895\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9892: 0s - loss: 0.0320 - ac\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0300 - accuracy: 0.9896\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0288 - accuracy: 0.9903: 1s - loss: 0.0261 - accuracy - E\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9911\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9910\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0234 - accuracy: 0.9921\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0242 - accuracy: 0.9918\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0223 - accuracy: 0.9923\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9931\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9929\n",
      "313/313 [==============================] - 0s 860us/step - loss: 0.0781 - accuracy: 0.9796\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3363 - accuracy: 0.9030\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1624 - accuracy: 0.9522\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.9637\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0973 - accuracy: 0.9709\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0837 - accuracy: 0.9746\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0726 - accuracy: 0.9774: 1s - loss: 0.0730 - accu - ETA: 0s - loss: 0.0709 - accura - ETA: 0s - los\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0643 - accuracy: 0.9800\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.9811\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.9828\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0485 - accuracy: 0.9847\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9859\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.9862\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9872: 1s - loss: 0.0376 -  - ETA: 0s -\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9886\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9890: 0s - loss:\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0308 - accuracy: 0.9897\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0311 - accuracy: 0.9895\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0275 - accuracy: 0.9909\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9906\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9911\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9911\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0236 - accuracy: 0.9918\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9926\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0237 - accuracy: 0.9917\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9929\n",
      "313/313 [==============================] - 0s 845us/step - loss: 0.0807 - accuracy: 0.9800\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3364 - accuracy: 0.9026\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1604 - accuracy: 0.9530\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1213 - accuracy: 0.9646\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0974 - accuracy: 0.9709\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0819 - accuracy: 0.9743\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0716 - accuracy: 0.9783\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0633 - accuracy: 0.9796\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0560 - accuracy: 0.9819\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0514 - accuracy: 0.9834\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0472 - accuracy: 0.9847\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0439 - accuracy: 0.9859\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0400 - accuracy: 0.9872\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0373 - accuracy: 0.9876\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0345 - accuracy: 0.9889\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0343 - accuracy: 0.9888\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0295 - accuracy: 0.9898\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0306 - accuracy: 0.9895\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0283 - accuracy: 0.9907\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9912\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0250 - accuracy: 0.9911\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0240 - accuracy: 0.9917\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9913\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0230 - accuracy: 0.9925\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9921\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9931\n",
      "313/313 [==============================] - 0s 860us/step - loss: 0.0681 - accuracy: 0.9812\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3412 - accuracy: 0.9021\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1625 - accuracy: 0.9527\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9637\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0990 - accuracy: 0.9699\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0852 - accuracy: 0.9739\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9771\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0645 - accuracy: 0.9800\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0592 - accuracy: 0.9811\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.9833\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0465 - accuracy: 0.9849\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0443 - accuracy: 0.9859\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0396 - accuracy: 0.9871\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0367 - accuracy: 0.9879\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9878\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0328 - accuracy: 0.9889\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0309 - accuracy: 0.9898\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0306 - accuracy: 0.9900\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0273 - accuracy: 0.9908\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0260 - accuracy: 0.9913\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0263 - accuracy: 0.9913\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0252 - accuracy: 0.9916\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0232 - accuracy: 0.9926\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9921\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0230 - accuracy: 0.9922\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9931\n",
      "313/313 [==============================] - 0s 946us/step - loss: 0.0748 - accuracy: 0.9816\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.3402 - accuracy: 0.9022\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1669 - accuracy: 0.9512\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1241 - accuracy: 0.9636\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1023 - accuracy: 0.9694\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0878 - accuracy: 0.9735\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0750 - accuracy: 0.9773\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.9787\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.9813\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0537 - accuracy: 0.9831\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9846\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9852\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9867\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9873\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9878\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9886\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0334 - accuracy: 0.9887\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9898\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0283 - accuracy: 0.9907\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0277 - accuracy: 0.9906\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9906\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0265 - accuracy: 0.9905\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0237 - accuracy: 0.9915\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9915\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9922\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9929\n",
      "313/313 [==============================] - 0s 952us/step - loss: 0.0785 - accuracy: 0.9807\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.9051: 0s - loss: 0.397\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1657 - accuracy: 0.9513\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1224 - accuracy: 0.9634\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1020 - accuracy: 0.9693\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0858 - accuracy: 0.9733\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9773\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0665 - accuracy: 0.9795\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0591 - accuracy: 0.9810\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0546 - accuracy: 0.9824\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0485 - accuracy: 0.9842\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0455 - accuracy: 0.9842\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0429 - accuracy: 0.9859\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9877\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9876\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9898: 0s - loss: 0.0314 - accuracy\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0335 - accuracy: 0.9888\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0304 - accuracy: 0.9896\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0280 - accuracy: 0.9903: 0s - loss: 0.026\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0277 - accuracy: 0.9905\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9905\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9913\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0233 - accuracy: 0.9924\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0227 - accuracy: 0.9922\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0213 - accuracy: 0.9928\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9923: 0s - loss: 0.0214 - \n",
      "313/313 [==============================] - 0s 917us/step - loss: 0.0736 - accuracy: 0.9815\n",
      "Epoch 1/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.3425 - accuracy: 0.9004\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1631 - accuracy: 0.9521\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9637\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0994 - accuracy: 0.9705\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0836 - accuracy: 0.9744\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0750 - accuracy: 0.9768\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0654 - accuracy: 0.9797\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0582 - accuracy: 0.9818: 0s - loss: 0.0579 - accuracy\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0543 - accuracy: 0.9827\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0487 - accuracy: 0.9850\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0449 - accuracy: 0.9856\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9864\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0391 - accuracy: 0.9866\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0365 - accuracy: 0.9877\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.0345 - accuracy: 0.9886\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0311 - accuracy: 0.9895\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0300 - accuracy: 0.9902\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0292 - accuracy: 0.9894\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0271 - accuracy: 0.9909\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9910\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0242 - accuracy: 0.9919\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9916\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0243 - accuracy: 0.9915\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9923\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0213 - accuracy: 0.9926\n",
      "313/313 [==============================] - 0s 832us/step - loss: 0.0768 - accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "all_tf_weights = list()\n",
    "for i in range(10):\n",
    "    all_tf_weights.append(get_tf_weights(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tf_weights = np.mean(all_tf_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 3/938 [00:00<00:28, 32.63it/s, loss=2.252, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 85.54it/s, loss=0.015, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 85.52it/s, loss=0.015, v_num=3]\n",
      "Testing:   2%|▏         | 17/938 [00:00<00:05, 169.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████▉| 937/938 [00:06<00:00, 123.93it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.008191703880836199, 'test_loss': 0.008191703880836199}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:06<00:00, 148.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   1%|          | 9/938 [00:00<00:13, 66.75it/s, loss=2.134, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 74.55it/s, loss=0.016, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 74.54it/s, loss=0.016, v_num=4]\n",
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 932/938 [00:05<00:00, 172.85it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.007606163811752636, 'test_loss': 0.007606163811752636}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 165.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 12/938 [00:00<00:11, 79.73it/s, loss=2.127, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:13<00:00, 71.89it/s, loss=0.015, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:13<00:00, 71.88it/s, loss=0.015, v_num=5]\n",
      "Testing:   1%|▏         | 12/938 [00:00<00:07, 119.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 931/938 [00:06<00:00, 141.98it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.007847462001889358, 'test_loss': 0.007847462001889358}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:06<00:00, 138.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   1%|          | 7/938 [00:00<00:16, 56.42it/s, loss=2.170, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 75.61it/s, loss=0.011, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 75.59it/s, loss=0.011, v_num=6]\n",
      "Testing:   1%|▏         | 13/938 [00:00<00:07, 128.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 930/938 [00:05<00:00, 155.24it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.006437031994640481, 'test_loss': 0.006437031994640481}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 159.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 11/938 [00:00<00:13, 70.74it/s, loss=2.111, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 73.03it/s, loss=0.023, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 73.01it/s, loss=0.023, v_num=7]\n",
      "Testing:   1%|▏         | 12/938 [00:00<00:07, 118.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▊| 924/938 [00:05<00:00, 176.22it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.007282857068880953, 'test_loss': 0.007282857068880953}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 160.81it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 11/938 [00:00<00:12, 73.70it/s, loss=2.102, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 73.06it/s, loss=0.012, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:12<00:00, 73.04it/s, loss=0.012, v_num=8]\n",
      "Testing:   1%|▏         | 12/938 [00:00<00:08, 115.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 930/938 [00:06<00:00, 121.01it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.008231987057416054, 'test_loss': 0.008231987057416054}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:06<00:00, 134.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 9/938 [00:00<00:14, 64.80it/s, loss=2.149, v_num=9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 88.98it/s, loss=0.020, v_num=9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 88.96it/s, loss=0.020, v_num=9]\n",
      "Testing:   2%|▏         | 17/938 [00:00<00:05, 168.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████▉| 936/938 [00:04<00:00, 189.46it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.007183375410660567, 'test_loss': 0.007183375410660567}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:04<00:00, 188.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   1%|▏         | 13/938 [00:00<00:10, 85.73it/s, loss=2.082, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 88.01it/s, loss=0.016, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 87.98it/s, loss=0.016, v_num=10]\n",
      "Testing:   2%|▏         | 17/938 [00:00<00:05, 163.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 928/938 [00:04<00:00, 187.14it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.006369947246236282, 'test_loss': 0.006369947246236282}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:04<00:00, 190.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   1%|▏         | 13/938 [00:00<00:10, 86.72it/s, loss=2.056, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 88.24it/s, loss=0.011, v_num=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 88.21it/s, loss=0.011, v_num=11]\n",
      "Testing:   2%|▏         | 15/938 [00:00<00:06, 149.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████▉| 937/938 [00:04<00:00, 188.83it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.00686777872997404, 'test_loss': 0.00686777872997404}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:04<00:00, 187.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 101 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   1%|▏         | 14/938 [00:00<00:10, 90.06it/s, loss=2.045, v_num=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 86.69it/s, loss=0.018, v_num=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 938/938 [00:10<00:00, 86.67it/s, loss=0.018, v_num=12]\n",
      "Testing:   2%|▏         | 17/938 [00:00<00:05, 164.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzvyagin/miniconda3/envs/resiliency/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 930/938 [00:05<00:00, 185.87it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_loss': 0.0066498999786107115, 'test_loss': 0.0066498999786107115}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 938/938 [00:05<00:00, 184.75it/s]\n"
     ]
    }
   ],
   "source": [
    "all_pt_weights = list()\n",
    "for i in range(10):\n",
    "    all_pt_weights.append(get_pt_weights(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6421d5e99c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_pt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pt_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3332\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3334\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3335\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/resiliency/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "mean_pt_weights = np.mean(all_pt_weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence(mean_tf_weights, mean_pt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
